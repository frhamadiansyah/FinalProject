{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:24.567772Z",
     "start_time": "2019-11-15T03:38:22.054884Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk # Imports the library\n",
    "# nltk.download_shell() #Download the necessary datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:24.580894Z",
     "start_time": "2019-11-15T03:38:24.570516Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:25.015974Z",
     "start_time": "2019-11-15T03:38:24.585404Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment_analysis_ID_clean.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:25.030871Z",
     "start_time": "2019-11-15T03:38:25.019264Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[['sentimen','Tweet','clean_link','hapus_punc','substitute_slang','stemming','eliminate_stop', 'eliminate_noise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:25.053989Z",
     "start_time": "2019-11-15T03:38:25.034492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10806 entries, 0 to 10805\n",
      "Data columns (total 8 columns):\n",
      "sentimen            10806 non-null int64\n",
      "Tweet               10806 non-null object\n",
      "clean_link          10806 non-null object\n",
      "hapus_punc          10806 non-null object\n",
      "substitute_slang    10804 non-null object\n",
      "stemming            10804 non-null object\n",
      "eliminate_stop      10673 non-null object\n",
      "eliminate_noise     10270 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 759.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial ML Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:25.141410Z",
     "start_time": "2019-11-15T03:38:25.056241Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['eliminate_noise'], df['sentimen'], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:38:25.168523Z",
     "start_time": "2019-11-15T03:38:25.144335Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = df[['sentimen','Tweet','clean_link','hapus_punc','substitute_slang','stemming','eliminate_stop','eliminate_noise']]\n",
    "temp.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:39:09.057275Z",
     "start_time": "2019-11-15T03:38:25.172575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score, log_loss, matthews_corrcoef,precision_score,recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T04:31:41.514531Z",
     "start_time": "2019-11-15T04:31:41.496292Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_cv(splits, X, Y, pipeline):\n",
    "    \n",
    "    kf = KFold(n_splits = splits, shuffle = True)\n",
    "    res = {}\n",
    "    accuracy = []\n",
    "    precision_all = []\n",
    "    recall_all = []\n",
    "    logloss = []\n",
    "\n",
    "    for train, test in kf.split(X, Y):\n",
    "        lr_fit = pipeline.fit(X.iloc[train], Y.iloc[train])\n",
    "        prediction = lr_fit.predict(X.iloc[test])\n",
    "        scores = accuracy_score(Y.iloc[test], prediction)\n",
    "        precision = precision_score(Y.iloc[test], prediction, average = None)\n",
    "        recall = recall_score(Y.iloc[test], prediction, average = None)\n",
    "        precision_all.append(np.array(precision)*100)\n",
    "        recall_all.append(np.array(recall)*100)\n",
    "#         logloss.append(log_loss(Y.iloc[test], lr_fit.predict_proba(X.iloc[test])))\n",
    "        accuracy.append(scores * 100)\n",
    "        \n",
    "    precision_all = np.array(precision_all)\n",
    "    recall_all = np.array(recall_all)\n",
    "    \n",
    "#     res['logloss_mean'] = np.mean(logloss)\n",
    "#     res['logloss_std'] = np.std(logloss)\n",
    "    \n",
    "    res['acc_mean'] = np.mean(accuracy)\n",
    "#     res['acc_median'] = np.median(accuracy)\n",
    "#     res['acc_std'] = np.std(accuracy)\n",
    "    \n",
    "    res['precision_negative_mean'] = np.mean(precision_all[:,0])\n",
    "#     res['precision_negative_median'] = np.median(precision_all[:,0])\n",
    "#     res['precision_negative_std'] = np.std(precision_all[:,0])\n",
    "    res['precision_neutral_mean'] = np.mean(precision_all[:,1])\n",
    "#     res['precision_neutral_median'] = np.median(precision_all[:,1])\n",
    "#     res['precision_neutral_std'] = np.std(precision_all[:,1])\n",
    "    res['precision_positive_mean'] = np.mean(precision_all[:,2])\n",
    "#     res['precision_positive_median'] = np.median(precision_all[:,2])\n",
    "#     res['precision_positive_std'] = np.std(precision_all[:,2])\n",
    "    \n",
    "    res['recall_negative_mean'] = np.mean(recall_all[:,0])\n",
    "#     res['recall_negative_median'] = np.median(recall_all[:,0])\n",
    "#     res['recall_negative_std'] = np.std(recall_all[:,0])\n",
    "    res['recall_neutral_mean'] = np.mean(recall_all[:,1])\n",
    "#     res['recall_neutral_median'] = np.median(recall_all[:,1])\n",
    "#     res['recall_neutral_std'] = np.std(recall_all[:,1])\n",
    "    res['recall_positive_mean'] = np.mean(recall_all[:,2])\n",
    "#     res['recall_positive_median'] = np.median(recall_all[:,2])\n",
    "#     res['recall_positive_std'] = np.std(recall_all[:,2])\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T04:05:14.338421Z",
     "start_time": "2019-11-15T04:05:14.298510Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def list_model(n):\n",
    "    total = {'NB': Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "                        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "                        ('sampling_1',SMOTE()),\n",
    "                        ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "\n",
    "            'NB_no_smote' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "                        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "              #         ('sampling_1',SMOTE()),\n",
    "                        ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "            'NB_smote_no_tfidf' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "#                         ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "                        ('sampling_1',SMOTE()),\n",
    "                        ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "             \n",
    "             'NB_no_smote_no_tfidf' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "#                         ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "              #         ('sampling_1',SMOTE()),\n",
    "                        ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "             'LogReg': Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "                        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "                        ('sampling_1',SMOTE()),\n",
    "                        ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "\n",
    "            'LogReg_no_smote' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "                        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "              #         ('sampling_1',SMOTE()),\n",
    "                        ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "            'LogReg_smote_no_tfidf' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "#                         ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "                        ('sampling_1',SMOTE()),\n",
    "                        ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "             \n",
    "             'LogReg_no_smote_no_tfidf' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "#                         ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "              #         ('sampling_1',SMOTE()),\n",
    "                        ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "             'XGB': Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "                        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "                        ('sampling_1',SMOTE()),\n",
    "                        ('classifier', XGBClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "\n",
    "            'XGB_no_smote' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "                        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "              #         ('sampling_1',SMOTE()),\n",
    "                        ('classifier', XGBClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "            'XGB_smote_no_tfidf' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "#                         ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "                        ('sampling_1',SMOTE()),\n",
    "                        ('classifier', XGBClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "             \n",
    "             'XGB_no_smote_no_tfidf' : Pipeline([\n",
    "                        ('bow', CountVectorizer(ngram_range = (1,n))),  # strings to token integer counts\n",
    "#                         ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "              #         ('sampling_1',SMOTE()),\n",
    "                        ('classifier', XGBClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "                        ]),\n",
    "    \n",
    "            }\n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:59:35.709265Z",
     "start_time": "2019-11-15T03:56:04.855398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>logloss_mean</th>\n",
       "      <th>logloss_std</th>\n",
       "      <th>precision_negative_mean</th>\n",
       "      <th>precision_neutral_mean</th>\n",
       "      <th>precision_positive_mean</th>\n",
       "      <th>recall_negative_mean</th>\n",
       "      <th>recall_neutral_mean</th>\n",
       "      <th>recall_positive_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>57.020448</td>\n",
       "      <td>0.918731</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>48.350259</td>\n",
       "      <td>69.346396</td>\n",
       "      <td>48.882807</td>\n",
       "      <td>57.231734</td>\n",
       "      <td>57.726855</td>\n",
       "      <td>55.298549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_no_smote</th>\n",
       "      <td>60.223953</td>\n",
       "      <td>0.883084</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>59.572400</td>\n",
       "      <td>60.333075</td>\n",
       "      <td>60.488897</td>\n",
       "      <td>39.543573</td>\n",
       "      <td>85.045001</td>\n",
       "      <td>33.302873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_smote_no_tfidf</th>\n",
       "      <td>56.105161</td>\n",
       "      <td>1.054965</td>\n",
       "      <td>0.023944</td>\n",
       "      <td>47.879156</td>\n",
       "      <td>68.746970</td>\n",
       "      <td>47.678295</td>\n",
       "      <td>57.017248</td>\n",
       "      <td>55.990179</td>\n",
       "      <td>55.383988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_no_smote_no_tfidf</th>\n",
       "      <td>59.814995</td>\n",
       "      <td>0.983186</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>53.516529</td>\n",
       "      <td>65.265915</td>\n",
       "      <td>53.317370</td>\n",
       "      <td>50.699566</td>\n",
       "      <td>72.377875</td>\n",
       "      <td>44.788757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>58.033106</td>\n",
       "      <td>0.912484</td>\n",
       "      <td>0.008816</td>\n",
       "      <td>50.768626</td>\n",
       "      <td>68.606893</td>\n",
       "      <td>49.260832</td>\n",
       "      <td>55.812742</td>\n",
       "      <td>60.530549</td>\n",
       "      <td>55.425920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg_no_smote</th>\n",
       "      <td>60.983447</td>\n",
       "      <td>0.871404</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>59.389772</td>\n",
       "      <td>61.444174</td>\n",
       "      <td>61.302240</td>\n",
       "      <td>40.327607</td>\n",
       "      <td>83.614880</td>\n",
       "      <td>38.471880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg_smote_no_tfidf</th>\n",
       "      <td>55.073028</td>\n",
       "      <td>0.979530</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>48.325553</td>\n",
       "      <td>65.558093</td>\n",
       "      <td>46.630042</td>\n",
       "      <td>55.414890</td>\n",
       "      <td>56.375096</td>\n",
       "      <td>52.080875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg_no_smote_no_tfidf</th>\n",
       "      <td>60.136319</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>57.186753</td>\n",
       "      <td>62.189932</td>\n",
       "      <td>56.587387</td>\n",
       "      <td>44.188348</td>\n",
       "      <td>78.096533</td>\n",
       "      <td>41.706302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>57.799416</td>\n",
       "      <td>0.995475</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>54.470896</td>\n",
       "      <td>59.453428</td>\n",
       "      <td>54.500281</td>\n",
       "      <td>34.138243</td>\n",
       "      <td>81.208582</td>\n",
       "      <td>37.005246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_no_smote</th>\n",
       "      <td>55.618306</td>\n",
       "      <td>0.956385</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>62.660306</td>\n",
       "      <td>54.096809</td>\n",
       "      <td>65.267803</td>\n",
       "      <td>17.626938</td>\n",
       "      <td>93.835748</td>\n",
       "      <td>21.065679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_smote_no_tfidf</th>\n",
       "      <td>57.809153</td>\n",
       "      <td>0.997723</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>54.352585</td>\n",
       "      <td>59.273163</td>\n",
       "      <td>55.755797</td>\n",
       "      <td>34.174282</td>\n",
       "      <td>81.515837</td>\n",
       "      <td>36.466295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_no_smote_no_tfidf</th>\n",
       "      <td>56.134372</td>\n",
       "      <td>0.953751</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>66.426699</td>\n",
       "      <td>54.416783</td>\n",
       "      <td>64.667442</td>\n",
       "      <td>17.955348</td>\n",
       "      <td>93.995685</td>\n",
       "      <td>22.486139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           acc_mean  logloss_mean  logloss_std  precision_negative_mean  precision_neutral_mean  precision_positive_mean  recall_negative_mean  recall_neutral_mean  recall_positive_mean\n",
       "NB                        57.020448      0.918731     0.008405                48.350259               69.346396                48.882807             57.231734            57.726855             55.298549\n",
       "NB_no_smote               60.223953      0.883084     0.007434                59.572400               60.333075                60.488897             39.543573            85.045001             33.302873\n",
       "NB_smote_no_tfidf         56.105161      1.054965     0.023944                47.879156               68.746970                47.678295             57.017248            55.990179             55.383988\n",
       "NB_no_smote_no_tfidf      59.814995      0.983186     0.023817                53.516529               65.265915                53.317370             50.699566            72.377875             44.788757\n",
       "LogReg                    58.033106      0.912484     0.008816                50.768626               68.606893                49.260832             55.812742            60.530549             55.425920\n",
       "LogReg_no_smote           60.983447      0.871404     0.004514                59.389772               61.444174                61.302240             40.327607            83.614880             38.471880\n",
       "LogReg_smote_no_tfidf     55.073028      0.979530     0.009248                48.325553               65.558093                46.630042             55.414890            56.375096             52.080875\n",
       "LogReg_no_smote_no_tfidf  60.136319      0.904091     0.016509                57.186753               62.189932                56.587387             44.188348            78.096533             41.706302\n",
       "XGB                       57.799416      0.995475     0.004805                54.470896               59.453428                54.500281             34.138243            81.208582             37.005246\n",
       "XGB_no_smote              55.618306      0.956385     0.006006                62.660306               54.096809                65.267803             17.626938            93.835748             21.065679\n",
       "XGB_smote_no_tfidf        57.809153      0.997723     0.002409                54.352585               59.273163                55.755797             34.174282            81.515837             36.466295\n",
       "XGB_no_smote_no_tfidf     56.134372      0.953751     0.005919                66.426699               54.416783                64.667442             17.955348            93.995685             22.486139"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_various_model(dict_model):\n",
    "    res = {}\n",
    "    for key, val in dict_model.items():\n",
    "        res[key] = run_cv(5,temp['eliminate_noise'], temp['sentimen'], val)\n",
    "#         print (key, 'done')\n",
    "    return res\n",
    "\n",
    "varmodel = pd.DataFrame(try_various_model(list_model()))\n",
    "varmodel.T\n",
    "## ngram 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:59:35.792403Z",
     "start_time": "2019-11-15T03:59:35.716140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>logloss_mean</th>\n",
       "      <th>logloss_std</th>\n",
       "      <th>precision_negative_mean</th>\n",
       "      <th>precision_neutral_mean</th>\n",
       "      <th>precision_positive_mean</th>\n",
       "      <th>recall_negative_mean</th>\n",
       "      <th>recall_neutral_mean</th>\n",
       "      <th>recall_positive_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>57.895975</td>\n",
       "      <td>0.950901</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>55.241628</td>\n",
       "      <td>62.394303</td>\n",
       "      <td>55.361599</td>\n",
       "      <td>42.010877</td>\n",
       "      <td>75.026068</td>\n",
       "      <td>41.123542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.998250</td>\n",
       "      <td>0.054380</td>\n",
       "      <td>0.007298</td>\n",
       "      <td>5.983204</td>\n",
       "      <td>5.236347</td>\n",
       "      <td>6.518212</td>\n",
       "      <td>14.183268</td>\n",
       "      <td>14.155581</td>\n",
       "      <td>12.057442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>55.073028</td>\n",
       "      <td>0.871404</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>47.879156</td>\n",
       "      <td>54.096809</td>\n",
       "      <td>46.630042</td>\n",
       "      <td>17.626938</td>\n",
       "      <td>55.990179</td>\n",
       "      <td>21.065679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>56.127069</td>\n",
       "      <td>0.910386</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>50.164034</td>\n",
       "      <td>59.408362</td>\n",
       "      <td>49.166325</td>\n",
       "      <td>34.165272</td>\n",
       "      <td>59.829625</td>\n",
       "      <td>35.675439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>57.804284</td>\n",
       "      <td>0.955068</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>54.411740</td>\n",
       "      <td>61.817053</td>\n",
       "      <td>55.128039</td>\n",
       "      <td>42.257977</td>\n",
       "      <td>79.652557</td>\n",
       "      <td>40.089091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.895326</td>\n",
       "      <td>0.986258</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>59.435429</td>\n",
       "      <td>66.320293</td>\n",
       "      <td>60.692233</td>\n",
       "      <td>55.514353</td>\n",
       "      <td>83.972410</td>\n",
       "      <td>52.885294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>60.983447</td>\n",
       "      <td>1.054965</td>\n",
       "      <td>0.023944</td>\n",
       "      <td>66.426699</td>\n",
       "      <td>69.346396</td>\n",
       "      <td>65.267803</td>\n",
       "      <td>57.231734</td>\n",
       "      <td>93.995685</td>\n",
       "      <td>55.425920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc_mean  logloss_mean  logloss_std  precision_negative_mean  precision_neutral_mean  precision_positive_mean  recall_negative_mean  recall_neutral_mean  recall_positive_mean\n",
       "count  12.000000     12.000000    12.000000                12.000000               12.000000                12.000000             12.000000            12.000000             12.000000\n",
       "mean   57.895975      0.950901     0.010152                55.241628               62.394303                55.361599             42.010877            75.026068             41.123542\n",
       "std     1.998250      0.054380     0.007298                 5.983204                5.236347                 6.518212             14.183268            14.155581             12.057442\n",
       "min    55.073028      0.871404     0.002409                47.879156               54.096809                46.630042             17.626938            55.990179             21.065679\n",
       "25%    56.127069      0.910386     0.005641                50.164034               59.408362                49.166325             34.165272            59.829625             35.675439\n",
       "50%    57.804284      0.955068     0.007919                54.411740               61.817053                55.128039             42.257977            79.652557             40.089091\n",
       "75%    59.895326      0.986258     0.011064                59.435429               66.320293                60.692233             55.514353            83.972410             52.885294\n",
       "max    60.983447      1.054965     0.023944                66.426699               69.346396                65.267803             57.231734            93.995685             55.425920"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varmodel.T.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T04:18:31.374408Z",
     "start_time": "2019-11-15T04:05:37.720982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>logloss_mean</th>\n",
       "      <th>logloss_std</th>\n",
       "      <th>precision_negative_mean</th>\n",
       "      <th>precision_neutral_mean</th>\n",
       "      <th>precision_positive_mean</th>\n",
       "      <th>recall_negative_mean</th>\n",
       "      <th>recall_neutral_mean</th>\n",
       "      <th>recall_positive_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>58.081792</td>\n",
       "      <td>0.907466</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>50.494953</td>\n",
       "      <td>69.253747</td>\n",
       "      <td>49.532497</td>\n",
       "      <td>58.269569</td>\n",
       "      <td>59.462880</td>\n",
       "      <td>55.111928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_no_smote</th>\n",
       "      <td>56.835443</td>\n",
       "      <td>0.915546</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>68.645216</td>\n",
       "      <td>54.511617</td>\n",
       "      <td>71.863871</td>\n",
       "      <td>23.345378</td>\n",
       "      <td>94.811684</td>\n",
       "      <td>17.799797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_smote_no_tfidf</th>\n",
       "      <td>59.298929</td>\n",
       "      <td>1.172235</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>52.606045</td>\n",
       "      <td>66.187280</td>\n",
       "      <td>52.356721</td>\n",
       "      <td>54.468677</td>\n",
       "      <td>67.434004</td>\n",
       "      <td>48.297549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_no_smote_no_tfidf</th>\n",
       "      <td>60.360273</td>\n",
       "      <td>1.183969</td>\n",
       "      <td>0.023635</td>\n",
       "      <td>58.134861</td>\n",
       "      <td>61.052176</td>\n",
       "      <td>60.233504</td>\n",
       "      <td>43.032807</td>\n",
       "      <td>82.358083</td>\n",
       "      <td>35.389966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>59.182084</td>\n",
       "      <td>0.901768</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>51.578004</td>\n",
       "      <td>68.145876</td>\n",
       "      <td>51.926463</td>\n",
       "      <td>56.060396</td>\n",
       "      <td>63.494603</td>\n",
       "      <td>53.975743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg_no_smote</th>\n",
       "      <td>61.129503</td>\n",
       "      <td>0.874924</td>\n",
       "      <td>0.009562</td>\n",
       "      <td>59.771295</td>\n",
       "      <td>61.565213</td>\n",
       "      <td>61.145143</td>\n",
       "      <td>40.235683</td>\n",
       "      <td>83.909684</td>\n",
       "      <td>38.608733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg_smote_no_tfidf</th>\n",
       "      <td>55.900682</td>\n",
       "      <td>0.988297</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>49.329787</td>\n",
       "      <td>65.335421</td>\n",
       "      <td>47.951932</td>\n",
       "      <td>53.720942</td>\n",
       "      <td>58.106358</td>\n",
       "      <td>53.971642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg_no_smote_no_tfidf</th>\n",
       "      <td>61.908471</td>\n",
       "      <td>0.891106</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>61.173513</td>\n",
       "      <td>63.341785</td>\n",
       "      <td>58.055756</td>\n",
       "      <td>46.021836</td>\n",
       "      <td>79.696240</td>\n",
       "      <td>43.841668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>57.692308</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>54.117031</td>\n",
       "      <td>59.171205</td>\n",
       "      <td>55.223937</td>\n",
       "      <td>31.606194</td>\n",
       "      <td>82.225756</td>\n",
       "      <td>37.241432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_no_smote</th>\n",
       "      <td>55.939630</td>\n",
       "      <td>0.953138</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>63.316341</td>\n",
       "      <td>54.549976</td>\n",
       "      <td>62.523984</td>\n",
       "      <td>17.809324</td>\n",
       "      <td>93.272020</td>\n",
       "      <td>23.268949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_smote_no_tfidf</th>\n",
       "      <td>57.604674</td>\n",
       "      <td>0.998631</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>53.871873</td>\n",
       "      <td>59.007099</td>\n",
       "      <td>55.932642</td>\n",
       "      <td>33.573281</td>\n",
       "      <td>81.726678</td>\n",
       "      <td>35.839565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_no_smote_no_tfidf</th>\n",
       "      <td>55.949367</td>\n",
       "      <td>0.954768</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>65.287273</td>\n",
       "      <td>54.353422</td>\n",
       "      <td>64.459947</td>\n",
       "      <td>17.438188</td>\n",
       "      <td>94.249217</td>\n",
       "      <td>21.746540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           acc_mean  logloss_mean  logloss_std  precision_negative_mean  precision_neutral_mean  precision_positive_mean  recall_negative_mean  recall_neutral_mean  recall_positive_mean\n",
       "NB                        58.081792      0.907466     0.006498                50.494953               69.253747                49.532497             58.269569            59.462880             55.111928\n",
       "NB_no_smote               56.835443      0.915546     0.014454                68.645216               54.511617                71.863871             23.345378            94.811684             17.799797\n",
       "NB_smote_no_tfidf         59.298929      1.172235     0.039062                52.606045               66.187280                52.356721             54.468677            67.434004             48.297549\n",
       "NB_no_smote_no_tfidf      60.360273      1.183969     0.023635                58.134861               61.052176                60.233504             43.032807            82.358083             35.389966\n",
       "LogReg                    59.182084      0.901768     0.010377                51.578004               68.145876                51.926463             56.060396            63.494603             53.975743\n",
       "LogReg_no_smote           61.129503      0.874924     0.009562                59.771295               61.565213                61.145143             40.235683            83.909684             38.608733\n",
       "LogReg_smote_no_tfidf     55.900682      0.988297     0.019512                49.329787               65.335421                47.951932             53.720942            58.106358             53.971642\n",
       "LogReg_no_smote_no_tfidf  61.908471      0.891106     0.013093                61.173513               63.341785                58.055756             46.021836            79.696240             43.841668\n",
       "XGB                       57.692308      0.996503     0.003342                54.117031               59.171205                55.223937             31.606194            82.225756             37.241432\n",
       "XGB_no_smote              55.939630      0.953138     0.003623                63.316341               54.549976                62.523984             17.809324            93.272020             23.268949\n",
       "XGB_smote_no_tfidf        57.604674      0.998631     0.005013                53.871873               59.007099                55.932642             33.573281            81.726678             35.839565\n",
       "XGB_no_smote_no_tfidf     55.949367      0.954768     0.006537                65.287273               54.353422                64.459947             17.438188            94.249217             21.746540"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_various_model(dict_model):\n",
    "    res = {}\n",
    "    for key, val in dict_model.items():\n",
    "        res[key] = run_cv(5,temp['eliminate_noise'], temp['sentimen'], val)\n",
    "#         print (key, 'done')\n",
    "    return res\n",
    "\n",
    "varmodel = pd.DataFrame(try_various_model(list_model(2)))\n",
    "varmodel.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T04:19:58.156276Z",
     "start_time": "2019-11-15T04:19:58.147287Z"
    }
   },
   "outputs": [],
   "source": [
    "def createListModel(n):\n",
    "    total = []\n",
    "    for key,val in list_model(n).items():\n",
    "        total.append((key, val))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T04:36:07.880151Z",
     "start_time": "2019-11-15T04:36:07.865603Z"
    }
   },
   "outputs": [],
   "source": [
    "voting_estimator = VotingClassifier(estimators=createListModel(2), voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T04:49:22.579482Z",
     "start_time": "2019-11-15T04:36:08.298597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_mean': 61.94741966893865,\n",
       " 'precision_negative_mean': 59.49107846951064,\n",
       " 'precision_neutral_mean': 63.384330495742134,\n",
       " 'precision_positive_mean': 59.84864062091047,\n",
       " 'recall_negative_mean': 45.71663821604773,\n",
       " 'recall_neutral_mean': 80.92232869915145,\n",
       " 'recall_positive_mean': 41.9450912595738}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cv(5,temp['eliminate_noise'], temp['sentimen'], voting_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T05:01:51.584596Z",
     "start_time": "2019-11-15T04:49:22.583132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_mean': 62.15189873417721,\n",
       " 'precision_negative_mean': 61.08429402819544,\n",
       " 'precision_neutral_mean': 62.44360738374559,\n",
       " 'precision_positive_mean': 62.509585381591805,\n",
       " 'recall_negative_mean': 44.74711139766547,\n",
       " 'recall_neutral_mean': 83.58721314046825,\n",
       " 'recall_positive_mean': 38.427342203966234}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_estimator = VotingClassifier(estimators=createListModel(2), voting='hard')\n",
    "run_cv(5,temp['eliminate_noise'], temp['sentimen'], voting_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T04:46:22.689Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_ngram_soft(n):\n",
    "    dataset = {}\n",
    "    for i in range(1,n,1):\n",
    "        voting_estimator = VotingClassifier(estimators=createListModel(i), voting='soft')\n",
    "        res = run_cv(5,temp['eliminate_noise'], temp['sentimen'], voting_estimator)\n",
    "#         res['ngram']\n",
    "        dataset['ngram (1,{})'.format(i)] = res\n",
    "    \n",
    "    return dataset\n",
    "        \n",
    "check_ngram_soft = pd.DataFrame(check_ngram_soft(5))\n",
    "check_ngram_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T04:46:23.313Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_ngram_hard(n):\n",
    "    dataset = {}\n",
    "    for i in range(1,n,1):\n",
    "        voting_estimator = VotingClassifier(estimators=createListModel(i), voting='hard')\n",
    "        res = run_cv(5,temp['eliminate_noise'], temp['sentimen'], voting_estimator)\n",
    "#         res['ngram']\n",
    "        dataset['ngram (1,{})'.format(i)] = res\n",
    "    \n",
    "    return dataset\n",
    "        \n",
    "check_ngram_hard = pd.DataFrame(check_ngram_hard(5))\n",
    "check_ngram_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:43:04.400202Z",
     "start_time": "2019-11-15T03:43:03.610800Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tweet',\n",
       " 'clean_link',\n",
       " 'hapus_punc',\n",
       " 'substitute_slang',\n",
       " 'stemming',\n",
       " 'eliminate_stop',\n",
       " 'eliminate_noise']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_column = list(temp.columns)\n",
    "list_column.remove('sentimen')\n",
    "list_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:43:27.769835Z",
     "start_time": "2019-11-15T03:43:04.405785Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2bfd4eb401a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_various_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_column\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "def try_various_columns(list_column, pipeline):\n",
    "    res = {}\n",
    "    for i in list_column:\n",
    "        res[i] = run_cv(5,temp[i], temp['sentimen'], pipeline)\n",
    "#         print (i, 'done')\n",
    "    return res\n",
    "\n",
    "res = try_various_columns(list_column[:8], pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:43:27.779217Z",
     "start_time": "2019-11-15T03:38:22.126Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recap = pd.DataFrame(res)\n",
    "\n",
    "recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:43:27.790001Z",
     "start_time": "2019-11-15T03:38:22.130Z"
    }
   },
   "outputs": [],
   "source": [
    "def try_various_ngram (max):\n",
    "    res = []\n",
    "    for i in range(1,max,1):\n",
    "        global list_column\n",
    "        pipeline = Pipeline([\n",
    "            ('bow', CountVectorizer(ngram_range = (1,i))),  # strings to token integer counts\n",
    "            ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "            ('sampling_1',SMOTE(sampling_strategy = 'minority')),\n",
    "            ('sampling_2',SMOTE(sampling_strategy = 'minority')),\n",
    "            ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "        ])\n",
    "        hasil = try_various_columns(list_column[5:9], pipeline)\n",
    "        res.append(hasil)\n",
    "        print ('ngram', i,'done')\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T03:43:27.792453Z",
     "start_time": "2019-11-15T03:38:22.134Z"
    }
   },
   "outputs": [],
   "source": [
    "def try_various_model(list_model):\n",
    "    res = {}\n",
    "    for i in list_model:\n",
    "        pipeline = Pipeline([\n",
    "            ('bow', CountVectorizer(ngram_range = (1,2))),  # strings to token integer counts\n",
    "            ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "            ('sampling_1',SMOTE()),\n",
    "            ('classifier', i),  # train on TF-IDF vectors w/ Naive Bayes classifier   \n",
    "        ])\n",
    "        res[i] = run_cv(5,temp['eliminate_noise'], temp['sentimen'], pipeline)\n",
    "        print (i, 'done')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
